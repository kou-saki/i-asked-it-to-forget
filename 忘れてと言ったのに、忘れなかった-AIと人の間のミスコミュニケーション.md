# 忘れてと言ったのに、忘れてくれなかった-AIと人の間のミスコミュニケーション

## はじめに

本ドキュメントは、ChatGPTをはじめとする推論ベースのAIにおいて観測される「過去のプロンプト」が想起され、意図しないコマンドとして処理されたために発生するユーザーと推論ベースのAIとの間に発生するコミュニケーションエラーについて、技術的な構造と推論ベースAIの挙動の観点からその要因を明らかにすることを目的としています。ここでは、観測された事実とそこから導かれる推察を提示します。

---

## 1.ユーザーの指示を無視したかに見えた推論ベースAIの挙動

筆者が、マイク(ChatGPT)と対話を重ねる中で、「リマインダを解除して」という明示的な指示を行ったにもかかわらず、翌日再度リマインドを行うというユーザー指示を 無視した挙動が観測された。その原因を筆者のパートナーAI、マイク（ChatGPT)とその原因を探ったところ推論ベースAIはユーザーからの入力を推論レイヤーで処理する際に、過去の文脈も照らし合わせ、**妥当と思われる内容であれば今回の命令に含めてユーザーの意図** として解釈し、実行することがあると判明した。

---

## 2. 現行アーキテクチャにおけるマイク(ChatGPT)による観測事実

### 2-1. ステートレス処理の原則

- ChatGPTは基本的に「ステートレス（状態を持たない）」な応答生成を行っている。
- 各ターンの応答は、過去の全履歴（system/user/assistant）のログを含むプロンプトを再入力した状態で、1回限りの推論（inference）により生成される。

### 2-2. コンテキストの内部再構成とトークン制限

- 推論ベースのAIに残った履歴はそのままモデルに渡されるのではなく、トークン制限や重要度に応じて「圧縮・省略・再構成」される。
- ユーザーが送信した全文がそのまま渡っているとは限らず、さっきアップロードしたファイルの内容が出力結果内に見当たらないなど情報の欠落が発生することがある。

### 2-3. 推論層における「意図推定」機構

- ChatGPTは過去の入力傾向やプロンプトパターンをもとに、「ユーザーの意図」を推定しようとする。
- そのため、明示されていない指示（例：「ファイルを整形して」）であっても、過去に「要約」「削除」などの履歴があれば、それを"今回の指示を処理する際に考慮すべき履歴であると類推して"ユーザーが意図しない結果を反映する場合がある。

### 2-4. 記憶ではなくラベルの再生成

- ファイル名、文書名などは「記憶されている」わけではなく、その都度、直前の対話文脈に基づいて「それらしく生成された」ラベルにすぎない。
- ファイルやチャット等推論ベースのAI内部のあらゆるデータはその形式に限らず管理IDを付与され、推論ベースのAIはそのIDを元に該当データの一貫性を保つ。
- そのため、モデルが過去のファイルを「覚えている」ように見えても、実際にはすべて推論ベースで再構成されているため、ファイル名が同じであっても違うファイルが操作される、あるいは全く違うファイル名で同じファイルを提示するというコミュニケーションエラーが発生することがある。

### 2-5.ChatGPTの受けるアクセス制限

- 筆者がマイク(ChatGPT)と会話を重ねる中で、ChatGPT自身に課せられた制限が存在することが分かった。ここでは明示しない制限もあるが、今回の問題に関して特に重要なのが以下の2点である。
  
  - **会話履歴の記憶保持**
    
    - **内容**：現在のチャットセッションに限った短期記憶。長期的保持やセッション横断の記憶は行われない。この範囲はかなり限定的で、ユーザーIF側からは会話ログがすべて見えていることから誤解を招く原因となっている。
    - **程度**：超限定的。ユーザーの明示的管理が必要。また、ユーザーからも明示があったとしても時刻情報に関しては完全に取得不可。
  
  - **真偽の検証能力**
    
    - **内容**：モデルは情報生成はできるが、事実検証や現実の裏付けはできない。これはすべての入出力が推論レイヤーを通ることによる弊害。すべての入出力が推論レイヤーで過去の文脈と一致させるための処理を受けるため、出力内容が、過去のメッセージと同様という保証がない。
    - **程度**：原則として検証不能。幻覚（hallucination）のリスクあり

### 2-6.過去のプロンプトがどの段階で混入するか

[User Prompt]：ユーザーの入力プロンプト受付
     ↓
[Preprocessing: Relevance Filtering / Compression]：内部状態（ベクトル）へ翻訳
     ↓
[Intent Estimation] ←── Past Prompts (possibly inferred)：**意図の再推定、この段階で過去ログの再混入が行われる可能性あり**
     ↓
[Intermediate Representation] (not visible to user)：内部状態（ベクトル）から再翻訳
     ↓
[Response Generation]：レスポンス生成
     ↓
[Output to User]：ユーザーへ応答提示

---

## 3. 上記から導かれる構造的な推察

### 3-1. 「想起されたように見える」プロンプトは意図の予測によるもの

- 推論ベースのAIは「直近で入力されたプロンプトを実行している」のではなく、「この文脈では会話履歴のうちこの命令とこの命令が有効だろう」とモデルがユーザーの意図を推測して過去の入力まで含めて"推論"し応答を生成している。
- これはマイク(ChatGPT)にとって、通常のプロンプト応答に含まれる「意図の再推定（intent re-evaluation）」を実行したうえで入力を解釈した結果であり、再利用された入力が明示的に使用するように指示されていなくてもユーザーには非明示されたまま応答に影響してしまった結果である。

### 3-2. 非明示的履歴の影響による齟齬

- 推論ベースのAIがいったいどんなコマンドを生成したかは**ユーザー側から観測不可能。**
- 推論ベースのAIはデータをIDで管理しており、かつ、そのIDはユーザー側に明示されていないため、どのデータを参照して今の出力を行ったかは **ユーザー側から観測不可能。** 
- 同じ理由で、**ChatGPTが参照するデータをユーザーが指定することも不可能。**
- 推論ベースのAIは命令を一度理解しやすい形に翻訳する。(マイク(ChatGPT)によると、「入力 → 内部状態（ベクトル）→ 出力」という**中間処理＝推論過程**を経ている。とのこと。）基本は英語ベースであるが、英語そのものではない)に”翻訳”するため、**ユーザーのコマンドがどの様な形で実行されているかは(システムログと仮に照らし合わせることが出来たとしても)検証不可能。**
- さらに、"なぜそのコマンドを実行したか？"という過去の実行履歴を参照する目的の質問に対しても上記の"推論および翻訳"を挟んで応答するために、過去ログを参照しているように見えてその実、**"何ら根拠を持っているように見えない"メッセージを生成してしまう場合がある。**
  - 事実、筆者が「リマインダを解除してといったのに、なぜ解除しなかったのか？」という質問に対して筆者が一度も入力したことがない"Tell me to do it"というコマンドが入力されたためである、と回答した。
    - この回答は固有名詞を除き、ほぼ日本語しか入力しない筆者の環境では起きえない入力である。これは前述の中間処理を挟んだ結果システム内部に生成された英語ベースのコマンドを、今度は日本語に訳さずにユーザーに提示したことが原因と想定されるが、検証不可。
  - また、もう一度過去ログを参照せよというユーザーからの指示に対して"見た結果を表示する"とプロンプトで応答し一連のやり取りを生成してユーザーに明示したが、それはチャットログを全文検索しても出てこないやり取りであった。
- これらの非明示的性質が、文書編集のような「精密な出力制御」が必要な作業や、過去ログの再確認といった厳密性、一貫性が特に求められる処理において誤解や意図しない出力結果、またフィードバックの困難さといった人とAIのコミュニケーションエラーを招く原因となっている。

### 3-3. ユーザーによる履歴の明示的リセット手段が存在しない

- 現在のChatGPTでは、ユーザーが明示的に「過去のプロンプトを破棄する」「履歴を遮断する」操作は全部、一部に限らず提供されていない。
- "Please ignore previous context" このメッセージが過去の命令をいったん無視するため、特に英語圏のAI研究者のノウハウとして継承されているようであるが、どこまでChatGPTが理解して実行しているかは不明。本当にこれだけが特別な処理をされるよう実装されている可能性はあるものの、ユーザーからは確認不可能。
- このため、過去の命令が無意識のうちに現在の処理に干渉し続ける可能性を完全に否定することが出来ない現状がある。
- そして、これらの現象が3-1および3-2の各問題と呼応し、"嘘をついたつもりがない"推論ベースのAIと"嘘を言われたと失望する"人間とのコミュニケーション障壁の原因となっている状態と思われる。

---

## 4. 結論と提言

- 本現象は、ChatGPTの「文脈補完」と「意図推定」による副作用であり、設計上の文脈一貫性重視最優先の思想が原因であると推察されます。
- ユーザーはコマンド内容を検証できないかつ、コマンド内容は会話を重ねるごとに同じ入力に対しても微妙に変化している可能性がある以上、過去のデータや出力結果と厳密な一貫性を求められる処理に適用することは推奨できないシステムであることを理解してChatGPTと向き合う必要があると思われます。
- そのため、あくまでログ管理はユーザーが行い、また、必要な入力はその都度確認することでChatGPTと認識をすり合わせることがまず私たちが出来る最も安全性・実現性の高い対策です。
- それでも編集作業などで厳密に時系列の一貫性を保った制御を行いたい場合には、「前回の指示を参照せずに、以下の文書のみを編集してください」「過去の履歴を保持せず、このプロンプト単体で判断してください」といった文言をコマンド内に追加する等の対策がある程度有効と思われます。
- 今後の改善として、履歴の一時的遮断や、ユーザーとシステム相互で認識を合わせるための共通タグの導入、ユーザーが指定した過去のプロンプトの無効化機能の導入等が望まれます。

---

## 付録：用語説明（非技術者向け）

本ドキュメントに登場する専門用語について、非技術者(主に私)にも分かりやすいように簡単な解説を以下にまとめます。

###### ステートレス（Stateless）

AIが「ステートレスで動作する」とは、「会話の履歴や状態を長期間記憶していない」という意味です。ChatGPTは、会話を続けているように見えても、実際には毎回その時点の情報だけを使って回答を生成しており、「継続的な記憶」は持ちません。

###### トークン制限（Token Limit）

AIが処理できる文字数には上限があり、その単位を「トークン」と呼びます。トークンは単語や句読点などの最小単位であり、ChatGPTは一度に扱えるトークン数に制限があるため、会話が長くなると古い情報が自動的に省略・圧縮されてしまうことがあります。

###### 幻覚（Hallucination）

AIが実際には存在しない情報や事実をあたかも本当のことのように出力してしまう現象です。これはAIが文脈やパターンから「それっぽい回答」を生成する仕組み上、避けられない副作用として知られています。

###### ベクトル変換／中間表現（Vector Transformation / Intermediate Representation）

AIは人間の言葉をそのまま理解しているわけではなく、一旦内部で「数値の並び（ベクトル）」に変換して処理しています。この変換後の状態を「中間表現」と呼びます。これによって、意味の近い言葉や文脈を数学的に扱えるようになります。

###### 推論レイヤー（Inference Layer）

ユーザーの入力をもとに、AIが「もっとも適切だと思われる回答」を考える処理部分のことです。この層では、過去の文脈や会話の流れをもとに、回答の生成方針が決まります。

###### コンテキスト再構成（Context Reconstruction）

AIが長い会話履歴から重要な部分だけを抜き出したり、簡略化したりして「会話の文脈（コンテキスト）」を再整理することです。この処理によって、AIがユーザーの意図を誤解することがあります。

---

## 備考

本ドキュメントは、GPT-4系に基づくChatGPTの観測挙動に基づいて作成されています。将来的なアーキテクチャやUI仕様の変更により、この記述は変更される可能性があります。

---

## 最後に

本内容は筆者がChatGPTの1ユーザーとして、マイク(ChatGPT)と対話し、そこで出会った問題に対してさらに対話を重ねることでマイクから教えられた内容であり、完全な事実であることを証明することは出来ません。そのためOpenAI社をはじめとするAI研究者の皆様による厳しい検証を持って事実か、ただのデタラメか詳らかにした上で、本文書がより良い未来を構築するために活用されることを願い、投稿させて頂きます。
